---
layout: post
title:  "The PIE framework sucks (but it’s the best we have)"
hero-header: "The best way I’ve found to manage A/B testing programs"
subhead: "Take a handful of guesstimates and them get the average. Surely there's a better way to prioritise your A/B testing program? I used to think that there was too."
excerpt: "Take a handful of guesstimates and them get the average. Surely there's a better way to prioritise your A/B testing program? I used to think that there was too."
categories: [blog]
permalink: "/pie-prioritisation-method-sucks"
slug: "2010-10-24-pie-prioritisation-method-sucks"
---
{% include image-large.html name="ocp-pie-troll.jpg" caption="Me anonymously trolling on the original PIE score blog post. As it turns out, I think I was both right and wrong." %}

## You need a prioritisation method


If you’re running more than one or two A/B tests a month, you need a formal prioritisation method. If you’re working with a team that’s bigger than just you, you need a prioritisation method. Actually, even if it’s just you, you still need a prioritisation method.


Formal prioritisation helps you to avoid waste time and energy on low impact, high effort tests. 

It makes sure that you focus on where you can get the best results for the least effort.

The process of prioritisation with a group has the added benefit of developing consensus and emotional investment in the direction of your testing program.

## How PIE scoring works

I’ve been using the PIE framework for years now. [First proposed by Chris Goward back in 2011](https://www.widerfunnel.com/how-to-prioritize-conversion-rate-optimization-tests-using-pie), it has since become the defacto standard for A/B test prioritisation.

It works like this:

You assign each test candidate a score out of ten according to the following criteria:

<ul class="list">
<li>Potential</li>
<li>Impact</li>
<li>Ease</li>
</ul>

After you’ve scored, you then take the average of the 3. 

The number that’s left over? That’s your PIE score.

Is it scientific? It is not.

Is it useful? It certainly is.

## Why PIE sucks

If you scroll down to the comments section at the bottom of the [Chris's original PIE blog post](https://www.widerfunnel.com/how-to-prioritize-conversion-rate-optimization-tests-using-pie), you’ll see a comment from one “OCP” (that that’s me, Oliver Charles Palmer in anonymous troll mode) basically calling bullshit on the whole endeavour:

> Isn’t the PIE score just an aggregation of a few guesstimates turned into a number out of ten?

To which Chris thoughtfully responded:

> As with many good strategic frameworks, it’s a mix of objective and subjective measures ... If it was all just data that could be pulled from an analytics report, we wouldn’t need talented strategists to do it. We could just write an algorithm! But, it’a not this simple.”

Chris is absolutely right.

Anonymous troll me is right, too, incidentally.

The thing about PIE is that it’s a quasi-quantitative meausure. 

You apply that number to represent the outcome of all your analysis and research and even a bit of instinct. 

## Why PIE is the best we have

By reducing ‘Potential’, ‘Impact’ and ’Ease’ to a number, you’re easily able to debate the relative merits of each test.

PIE really comes into it’s own is where you’re running an optimisation program with a group of different stakeholders with competing interests.

I get everyone involved in the program info a meeting room once a month and we sit down and debate the PIE scores for each test.

At the end of the session, everyone has a clear idea about what we’re working on next and why. In that regard, PIE is an essential tool for helping to establish consensus.

## Surely there's something better?

I used to look around occasionally to see if anyone had come up with a better method of prioritisation.

And people do try, occasionally,

In most cases, they are vain attempts to make prioritisation ‘more scientific’. 

They’ll add umpteen different data points (“Is the test creative above the fold?” and “Does the test add new information to the page?” and so on) in an effort to give their method some quasi scientific legitimacy.

I think this is utterly too complicated and maddening and silly.

PIE appears crude and simple but that simplicity is actually an elegant feature not a bug. 

By reducing the inputs to three scores, it provides a simple and accessible framework for discussion and debate.

It doesn’t try to be scientific. 

As Chris indicates in his comment above, he was absolutely aware that creating a scientific arbiter of impact and effort would be a quixotic struggle.

Instead, in its beautiful simplicity, PIE puts the onus on us to ask the right questions. It gives us a framework for debate and discussion and consensus building.

So yeah, if you’re looking for a magic number crunching test prioritisation machine, PIE sucks. 

For the rest of us, however, it’s an invaluable tool. 

Thanks for sharing it, Chris and sorry for trolling you ;-) 


